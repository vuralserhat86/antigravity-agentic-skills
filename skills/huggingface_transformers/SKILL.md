---
name: huggingface_transformers
router_kit: AIKit
description: Hugging Face Transformers library usage, model selection, fine-tuning ve deployment rehberi.
metadata:
  skillport:
    category: ai
    tags: [agents, algorithms, artificial intelligence, automation, chatbots, cognitive services, deep learning, embeddings, frameworks, generative ai, huggingface transformers, inference, large language models, llm, machine learning, model fine-tuning, natural language processing, neural networks, nlp, openai, prompt engineering, rag, retrieval augmented generation, tools, vector databases, workflow automation]      - prompt-engineering
---

# ğŸ¤— Hugging Face Transformers

> Transformer modellerini (NLP, Vision, Audio) kullanma ve ince ayar (fine-tuning) rehberi.

---

## ğŸ—ï¸ Core components

### 1. Pipelines (Quick Start)
En basit kullanÄ±m yolu. Model ve tokenizer otomatik yÃ¼klenir.

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
result = classifier("I love using Hugging Face!")[0]
print(f"Label: {result['label']}, Score: {result['score']}")
```

### 2. AutoClasses (Manual Control)
Model ve tokenizer'Ä± manuel seÃ§mek iÃ§in:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

---

## ğŸ¨ Common Tasks

| GÃ¶rev | Pipeline AdÄ± |
|-------|--------------|
| Text Classification | `text-classification` |
| Text Generation | `text-generation` |
| Summarization | `summarization` |
| Translation | `translation` |
| Object Detection | `object-detection` |

---

## ğŸ”§ Optimization & Deployment

- **Quantization**: 4-bit/8-bit yÃ¼kleme (BitsAndBytes) ile RAM kullanÄ±mÄ± azaltma.
- **ONNX/TensorRT**: Ãœretim ortamÄ±nda hÄ±zlandÄ±rma.
- **PEFT/LoRA**: Ã‡ok daha az parametre ile verimli ince ayar (fine-tuning).

---

*Hugging Face Transformers v1.1 - Enhanced*

## ğŸ”„ Workflow

> **Kaynak:** [Hugging Face Course](https://huggingface.co/course) & [Production Guide](https://huggingface.co/docs/transformers/performance)

### AÅŸama 1: Model Selection
- [ ] **Task**: GÃ¶revine en uygun mimariyi seÃ§ (Encoder: classification, Decoder: generation).
- [ ] **License**: Modelin ticari kullanÄ±m izni (Apache 2.0 vs Llama Community) var mÄ±?
- [ ] **Size**: Parametre sayÄ±sÄ± vs performans dengesini kur (7B genellikle yeterli).

### AÅŸama 2: Optimization pipeline
- [ ] **Quantization**: Inference iÃ§in 4-bit / 8-bit quantization (BitsAndBytes) kullan.
- [ ] **Batching**: Tek tek deÄŸil, batch halinde process et (GPU verimi).
- [ ] **Format**: Production iÃ§in ONNX veya TensorRT formatÄ±na Ã§evir.

### AÅŸama 3: Deployment
- [ ] **Cache**: Model aÄŸÄ±rlÄ±klarÄ±nÄ± ve tokenizer'Ä± docker image iÃ§ine bake etme, volume kullan.
- [ ] **Token Limits**: Context window sÄ±nÄ±rÄ±nÄ± aÅŸan inputlar iÃ§in strateji belirle (chunking).

### Kontrol NoktalarÄ±
| AÅŸama | DoÄŸrulama |
|-------|-----------|
| 1 | Model GPU hafÄ±zasÄ±na sÄ±ÄŸÄ±yor mu (OOM hatasÄ±)? |
| 2 | Inference sÃ¼resi (Latency) hedefin altÄ±nda mÄ±? |
| 3 | Tokenizer ile Model uyumlu mu (aynÄ± vocab)? |
